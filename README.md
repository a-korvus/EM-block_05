# Парсинг информации и выборочное получение из базы данных

## Задачи

* Парсер
* Получение отфильтрованных данных
* Кэширование результатов запросов
* Очистка кэша по расписанию
* Объединение всего функционала в одно приложение на базе FastAPI

### Парсер

#### Задача

Написать парсер, который скачивает бюллетень по итогам торгов с [сайта биржи](https://spimex.com/markets/oil_products/trades/results/).

* *Достать из бюллетеней необходимые столбцы*

  Забрать только данные из таблицы «Единица измерения: Метрическая тонна», где по столбцу «Количество Договоров, шт.» значения больше 0:

  1. Код инструмента `exchange_product_id`
  2. Наименование инструмента `exchange_product_name`
  3. Базис поставки `delivery_basis_name`
  4. Объем договоров в единицах измерения `volume`
  5. Объем договоров, руб `total`
  6. Количество договоров, шт `count`

* *Сохранить полученные данные в таблицу `spimex_trading_results` со следующей структурой:*
  1. `id`
  2. `exchange_product_id`
  3. `exchange_product_name`
  4. `oil_id` - `exchange_product_id[:4]`
  5. `delivery_basis_id` - `exchange_product_id[4:7]`
  6. `delivery_basis_name`
  7. `delivery_type_id` - `exchange_product_id[-1]`
  8. `volume`
  9. `total`
  10. `count`
  11. `date`
  12. `created_on`
  13. `updated_on`

* *Создать базу данных, которая будет хранить информацию по итогам торгов начиная с 2023 года.*
* *Для подключения к PostgreSQL использовать асинхронную сессию*

#### Реализация

Весь код парсера находится в отдельном [модуле](./app/scraper/).
Скрапинг реализован следующими этапами:

1. Парсинг списка ссылок на файлы
2. Скачивание файлов во временную директорию
3. Извлечение необходимых данных из файлов
4. Сохранение извлеченной информации в базу данных и очистка временной директории

Все этапы реализованы асинхронно. Основной поток не блокируется при скрапинге. Для CPU-bound задач обработки файлов с помощью pandas предусмотрен многопроцессорный подход.

### Получение отфильтрованных данных

#### Задача

Реализовать микросервисное приложение, которые будет использовать данные из таблицы «spimex_trading_results» и отдавать их в формате JSON.

*Функции для реализации*:

* **`get_last_trading_dates`** – список дат последних торговых дней (фильтрация по количеству последних торговых дней);

* **`get_dynamics`** – список торгов за заданный период (фильтрация по `oil_id`, `delivery_type_id`, `delivery_basis_id`, `start_date`, `end_date`);

* **`get_trading_results`** – список последних торгов (фильтрация по `oil_id`, `delivery_type_id`, `delivery_basis_id`).

*Какие параметры должны быть обязательные, а какие нет, необходимо определить самостоятельно и обосновать.*

#### Реализация

* Все обработчики расположены в одном из файлов [маршрутов](./app/routers/get_data.py);
* После запуска контейнеризированного приложения необходимо перейти по адресу [http://0.0.0.0/docs](http://0.0.0.0/docs). Управление приложением можно осуществить из интерактивной документации OpenAPI;
* Перед проверкой работы эндпоинтов, следует запустить скрапер [http://0.0.0.0/docs#/scraper/start_scrap_start_scrap__get](http://0.0.0.0/docs#/scraper/start_scrap_start_scrap__get);
* Первый запуск займет порядка 40ка секунд. Однако основной поток не заблокирован и приложение продолжает отвечать на запросы, как того и ождается от асинхронной реализации. В логах контейнера можно увидеть информационные сообщения об этапах выполнения скрапинга. По завершении его работы, в логах будет сообщение, которое содержит общее время выполнения: *The scraper worked in 40.5670 seconds.*;
* Если другой клиент попытается запустить скрапер в процессе работы уже запущенного, он не сможет этого сделать, не дождавшись окончания деятельности предыдыдущего. Это ограничение установлено с помощью примитива синхронизации `asyncio.Event` во избежание заполнения БД одинаковыми данными, полученными в результате одновременной работы нескольких парсеров, запущенных разными клиентами;
* Последующие запуски скрапера будут скачивать и обрабатывать только новые файлы, которые добавились на сайт. Если, конечно, такие файлы будут на момент запуска парсинга. Поэтому их обработка будет занимать доли секунд.

### Кэширование результатов запросов

#### Задача

Необходимо организовать кэширование запросов

#### Реализация

Кэширование реализовано кастомным способом. В качестве хранилища кэша используется **Redis**. Вся логика работы с кэшем находится в одном [файле](./app/utils/caching.py). При необходимости кэшировать результат запроса, выполняются следующие действия:

* Пробуем получить данные из кэша по ключу в методе `get_cache_data`;
* Если обработчик возвращает какой-то результат, значит, кэширование этого запроса было произведено ранее и передаем этот результат клиенту;
* При отсутствии результата работы `get_cache_data`, выполняем запрос к базе данных в соответствии с требованиями клиента и сохраняем полученный результат в кэш с помощью метода `set_cache_data`. После чего также отвечаем клиенту данными, которые только что получили из БД и кэшировали.

### Очистка кэша по расписанию

#### Задача

Необходимо хранить кэшированные запросы до 14:11, а после сбрасывать весь кэш.

#### Реализация

* Для выполнения задачи очистки кэша по расписанию используется Celery. Логика работы расположена в отдельном [модуле](./app/background/);
* Корректный запуск задачи очистки кэша обеспечивается запуском *celery beat* + *crontab*.

### Объединение всего функционала в одно приложение на базе FastAPI

#### Задача

Обеспечить согласованную работу всех компонентов приложения. Централизованно запускать сервисы и управлять всеми доступными операциями.

#### Реализация

* В качестве централизованного запуска сервисов используется docker и docker compose;
* Все сервисы их настройки описаны в [docker-compose.yaml](./docker-compose.yaml);
* Сервисы с FastAPI приложением, Celery worker и Celery beat собираются из одного [Dockerfile](./Dockerfile)
* Контейнеры работают в одной сети. Перед бэкендом фастапи работает nginx.

##### Порядок запуска

1. Создать файл `.env` в корне проекта (на уровне с *requirements.txt*), заполнить его по образцу `.env.example`;
2. Создать докер-сеть (если еще не создана) командой `docker network create task_net`;
3. Запустить сборку и запуск контейнеров командой `docker compose up --build`. При таком запуске окно терминала будет занято запущенными контейнерами и можно сразу мониторить все логи со всех контейнеров;
4. При необходимости независимого от терминала запуска приложения, используем `docker compose up -d --build`
